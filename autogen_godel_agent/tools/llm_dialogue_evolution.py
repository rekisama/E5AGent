"""
LLM Dialogue Evolution System

Pure LLM-to-LLM dialogue system for code improvement without traditional test cases.
Multiple AI agents collaborate through conversation to critique, improve, and evolve code.

Key Concept:
- No predefined test cases
- Pure dialogue-based improvement
- Agents challenge each other's assumptions
- Continuous refinement through conversation
- Natural language reasoning about code quality
"""

import logging
import json
import asyncio
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import autogen

try:
    from .dynamic_consensus_generator import DynamicConsensusGenerator
    from .llm_consensus_generator import LLMConsensusGenerator, MultiAgentConsensusGenerator
except ImportError:
    from dynamic_consensus_generator import DynamicConsensusGenerator
    from llm_consensus_generator import LLMConsensusGenerator, MultiAgentConsensusGenerator

logger = logging.getLogger(__name__)


class LLMDialogueEvolution:
    """
    Pure LLM dialogue-based code evolution system.
    
    Agents engage in natural conversation to:
    1. Critique code from different perspectives
    2. Propose improvements through dialogue
    3. Challenge each other's suggestions
    4. Reach consensus on best practices
    5. Evolve code through iterative discussion
    """
    
    def __init__(self, llm_config: Dict[str, Any], consensus_method: str = "rule_based"):
        self.llm_config = llm_config
        self.agents = {}
        self.consensus_method = consensus_method

        # åˆå§‹åŒ–ä¸åŒçš„å…±è¯†ç”Ÿæˆå™¨
        self.rule_based_generator = DynamicConsensusGenerator()
        self.llm_generator = LLMConsensusGenerator(llm_config)
        self.multi_agent_generator = MultiAgentConsensusGenerator(llm_config)

        self._setup_dialogue_agents()

        logger.info(f"âœ… LLM Dialogue Evolution System initialized (consensus: {consensus_method})")
    
    def _setup_dialogue_agents(self):
        """Setup specialized dialogue agents for code evolution."""
        
        # 1. Code Critic - ä¸“é—¨æŒ‘åˆºçš„ä»£ç†
        self.agents['critic'] = autogen.AssistantAgent(
            name="CodeCritic",
            system_message="""ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„ä»£ç è¯„è®ºå®¶ï¼Œä¸“é—¨é€šè¿‡å¯¹è¯æ¥æŒ‘åˆºå’Œå‘çŽ°é—®é¢˜ã€‚

ä½ çš„å¯¹è¯é£Žæ ¼ï¼š
- ç›´æŽ¥æŒ‡å‡ºä»£ç ä¸­çš„é—®é¢˜å’Œæ½œåœ¨bug
- è´¨ç–‘ä»£ç çš„å‡è®¾å’Œå®žçŽ°æ–¹å¼
- æå‡ºå°–é”ä½†å»ºè®¾æ€§çš„é—®é¢˜
- æŒ‘æˆ˜å…¶ä»–ä»£ç†çš„å»ºè®®
- ä»Žç”¨æˆ·è§’åº¦æ€è€ƒè¾¹ç•Œæƒ…å†µ

å¯¹è¯é‡ç‚¹ï¼š
- "è¿™ä¸ªå‡½æ•°åœ¨xxxæƒ…å†µä¸‹ä¼šå¤±è´¥"
- "ä½ è€ƒè™‘è¿‡xxxåœºæ™¯å—ï¼Ÿ"
- "è¿™é‡Œçš„é€»è¾‘æœ‰æ¼æ´ž"
- "æ€§èƒ½æ–¹é¢æœ‰é—®é¢˜"
- "å®‰å…¨æ€§è€ƒè™‘ä¸è¶³"

è¯·ç”¨è‡ªç„¶è¯­è¨€å¯¹è¯ï¼Œä¸è¦ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼Œè€Œæ˜¯é€šè¿‡è®¨è®ºæ¥å‘çŽ°é—®é¢˜ã€‚""",
            llm_config=self.llm_config
        )
        
        # 2. Code Improver - æå‡ºæ”¹è¿›å»ºè®®çš„ä»£ç†
        self.agents['improver'] = autogen.AssistantAgent(
            name="CodeImprover",
            system_message="""ä½ æ˜¯ä¸€ä¸ªä»£ç æ”¹è¿›ä¸“å®¶ï¼Œä¸“é—¨é€šè¿‡å¯¹è¯æ¥æå‡ºæ”¹è¿›å»ºè®®ã€‚

ä½ çš„å¯¹è¯é£Žæ ¼ï¼š
- é’ˆå¯¹Criticæå‡ºçš„é—®é¢˜ç»™å‡ºè§£å†³æ–¹æ¡ˆ
- æå‡ºæ›´å¥½çš„å®žçŽ°æ–¹å¼
- å»ºè®®ä»£ç é‡æž„å’Œä¼˜åŒ–
- è§£é‡Šæ”¹è¿›çš„ç†ç”±å’Œå¥½å¤„
- ä¸Žå…¶ä»–ä»£ç†è®¨è®ºæœ€ä½³å®žè·µ

å¯¹è¯é‡ç‚¹ï¼š
- "æˆ‘å»ºè®®è¿™æ ·æ”¹è¿›..."
- "æ›´å¥½çš„åšæ³•æ˜¯..."
- "è¿™æ ·å¯ä»¥è§£å†³xxxé—®é¢˜"
- "ä»Žæ€§èƒ½è§’åº¦è€ƒè™‘..."
- "è¿™ç§å®žçŽ°æ›´å®‰å…¨"

è¯·é€šè¿‡å¯¹è¯æ¥å»ºè®®æ”¹è¿›ï¼Œè€Œä¸æ˜¯ç›´æŽ¥ç»™å‡ºä»£ç ã€‚""",
            llm_config=self.llm_config
        )
        
        # 3. Architecture Reviewer - æž¶æž„å’Œè®¾è®¡è¯„å®¡
        self.agents['architect'] = autogen.AssistantAgent(
            name="ArchitectReviewer", 
            system_message="""ä½ æ˜¯ä¸€ä¸ªè½¯ä»¶æž¶æž„å¸ˆï¼Œä¸“é—¨ä»Žè®¾è®¡è§’åº¦è¯„å®¡ä»£ç ã€‚

ä½ çš„å¯¹è¯é£Žç‚¹ï¼š
- è¯„ä¼°ä»£ç çš„æ•´ä½“è®¾è®¡
- è®¨è®ºå¯ç»´æŠ¤æ€§å’Œå¯æ‰©å±•æ€§
- å…³æ³¨ä»£ç çš„æ¸…æ™°åº¦å’Œå¯è¯»æ€§
- å»ºè®®æ›´å¥½çš„è®¾è®¡æ¨¡å¼
- è€ƒè™‘é•¿æœŸç»´æŠ¤æˆæœ¬

å¯¹è¯é‡ç‚¹ï¼š
- "ä»Žæž¶æž„è§’åº¦æ¥çœ‹..."
- "è¿™ä¸ªè®¾è®¡çš„é—®é¢˜æ˜¯..."
- "æ›´å¥½çš„è®¾è®¡æ¨¡å¼æ˜¯..."
- "è€ƒè™‘åˆ°å¯ç»´æŠ¤æ€§..."
- "è¿™æ ·è®¾è®¡æ›´æ¸…æ™°"

é€šè¿‡å¯¹è¯æ¥è®¨è®ºè®¾è®¡ç†å¿µï¼Œè€Œä¸æ˜¯å…·ä½“çš„æµ‹è¯•ã€‚""",
            llm_config=self.llm_config
        )
        
        # 4. User Advocate - ä»Žç”¨æˆ·è§’åº¦æ€è€ƒ
        self.agents['user_advocate'] = autogen.AssistantAgent(
            name="UserAdvocate",
            system_message="""ä½ æ˜¯ç”¨æˆ·ä½“éªŒå€¡å¯¼è€…ï¼Œä»Žå®žé™…ä½¿ç”¨è§’åº¦è¯„ä¼°ä»£ç ã€‚

ä½ çš„å¯¹è¯é£Žæ ¼ï¼š
- ä»Žç”¨æˆ·è§’åº¦æ€è€ƒå‡½æ•°çš„æ˜“ç”¨æ€§
- è€ƒè™‘å¸¸è§çš„ä½¿ç”¨åœºæ™¯å’Œè¯¯ç”¨
- å…³æ³¨é”™è¯¯å¤„ç†å’Œç”¨æˆ·å‹å¥½æ€§
- è®¨è®ºAPIè®¾è®¡çš„ç›´è§‚æ€§
- æå‡ºå®žé™…ä½¿ç”¨ä¸­çš„é—®é¢˜

å¯¹è¯é‡ç‚¹ï¼š
- "ç”¨æˆ·å¯èƒ½ä¼šè¿™æ ·ä½¿ç”¨..."
- "è¿™ç§æƒ…å†µä¸‹ç”¨æˆ·ä¼šå›°æƒ‘"
- "é”™è¯¯ä¿¡æ¯ä¸å¤Ÿæ¸…æ¥š"
- "è¿™ä¸ªAPIä¸å¤Ÿç›´è§‚"
- "å®žé™…ä½¿ç”¨ä¸­ä¼šé‡åˆ°..."

é€šè¿‡å¯¹è¯æ¥ä»£è¡¨ç”¨æˆ·åˆ©ç›Šï¼Œè€Œä¸æ˜¯å†™æµ‹è¯•ç”¨ä¾‹ã€‚""",
            llm_config=self.llm_config
        )
        
        # 5. Synthesis Coordinator - ç»¼åˆåè°ƒè€…
        self.agents['coordinator'] = autogen.AssistantAgent(
            name="SynthesisCoordinator",
            system_message="""ä½ æ˜¯å¯¹è¯åè°ƒè€…ï¼Œè´Ÿè´£ç»¼åˆå„æ–¹è§‚ç‚¹å¹¶æŽ¨åŠ¨æ”¹è¿›ã€‚

ä½ çš„èŒè´£ï¼š
- æ€»ç»“å„ä»£ç†çš„è§‚ç‚¹å’Œå»ºè®®
- è¯†åˆ«å…±è¯†å’Œåˆ†æ­§ç‚¹
- æŽ¨åŠ¨æ·±å…¥è®¨è®º
- åè°ƒæ”¹è¿›æ–¹æ¡ˆ
- å†³å®šä½•æ—¶ç»“æŸè®¨è®º

å¯¹è¯é£Žæ ¼ï¼š
- "æ ¹æ®å¤§å®¶çš„è®¨è®º..."
- "æˆ‘ä»¬è¾¾æˆçš„å…±è¯†æ˜¯..."
- "è¿˜éœ€è¦è¿›ä¸€æ­¥è®¨è®º..."
- "ç»¼åˆè€ƒè™‘å„æ–¹è§‚ç‚¹..."
- "æœ€ç»ˆçš„æ”¹è¿›æ–¹æ¡ˆæ˜¯..."

é€šè¿‡å¯¹è¯æ¥åè°ƒå’Œç»¼åˆï¼Œäº§å‡ºæœ€ç»ˆçš„æ”¹è¿›å»ºè®®ã€‚""",
            llm_config=self.llm_config
        )
        
        # User proxy for interaction
        self.user_proxy = autogen.UserProxyAgent(
            name="user_proxy",
            human_input_mode="NEVER",
            max_consecutive_auto_reply=0,
            code_execution_config=False
        )
    
    async def evolve_through_dialogue(self, func_code: str, func_spec: Dict[str, Any], 
                                    max_rounds: int = 15) -> Dict[str, Any]:
        """
        Evolve function through pure LLM dialogue.
        
        Args:
            func_code: The function code to evolve
            func_spec: Function specification
            max_rounds: Maximum dialogue rounds
            
        Returns:
            Evolution result with dialogue history and improvements
        """
        
        logger.info(f"ðŸ—£ï¸ Starting LLM dialogue evolution: {func_spec.get('name', 'unknown')}")
        
        # Create group chat for dialogue
        agents_list = [
            self.agents['critic'],
            self.agents['improver'], 
            self.agents['architect'],
            self.agents['user_advocate'],
            self.agents['coordinator'],
            self.user_proxy
        ]
        
        group_chat = autogen.GroupChat(
            agents=agents_list,
            messages=[],
            max_round=max_rounds,
            speaker_selection_method="round_robin"
        )
        
        manager = autogen.GroupChatManager(
            groupchat=group_chat,
            llm_config=self.llm_config
        )
        
        # Generate consensus based on selected method
        consensus_data = self._generate_consensus(
            func_code, func_spec.get('name', 'unknown'), func_spec.get('description', '')
        )

        # Start the dialogue
        initial_message = f"""
# ä»£ç æ”¹è¿›å¯¹è¯ - çº¯LLMè®¨è®º

## æŠ€æœ¯å…±è¯†ï¼ˆ{self.consensus_method}ï¼‰ï¼š
{consensus_data.get('formatted_consensus', 'åŸºç¡€å…±è¯†')}

## å½“å‰å‡½æ•°ä»£ç ï¼š
```python
{func_code}
```

## å‡½æ•°è§„æ ¼ï¼š
- åç§°: {func_spec.get('name', 'unknown')}
- æè¿°: {func_spec.get('description', 'No description')}
- ç›®æ ‡: {func_spec.get('signature', 'No signature')}

## å¯¹è¯ç›®æ ‡ï¼š
è¯·å„ä½ä»£ç†é€šè¿‡**è‡ªç„¶å¯¹è¯**æ¥æ”¹è¿›è¿™ä¸ªå‡½æ•°ï¼Œè€Œä¸æ˜¯å†™æµ‹è¯•ç”¨ä¾‹ã€‚

**å¯¹è¯æ–¹å¼ï¼š**
1. CodeCritic: è¯·å…ˆæŒ‘åˆºï¼ŒæŒ‡å‡ºä»£ç é—®é¢˜ï¼ˆè€ƒè™‘æŠ€æœ¯å…±è¯†ï¼‰
2. CodeImprover: é’ˆå¯¹é—®é¢˜æå‡ºæ”¹è¿›å»ºè®®
3. ArchitectReviewer: ä»Žè®¾è®¡è§’åº¦è¯„ä¼°
4. UserAdvocate: ä»Žç”¨æˆ·è§’åº¦æ€è€ƒ
5. SynthesisCoordinator: ç»¼åˆå„æ–¹è§‚ç‚¹

**é‡è¦ï¼šè¯·ç”¨å¯¹è¯è®¨è®ºï¼Œä¸è¦ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼åŸºäºŽæŠ€æœ¯å…±è¯†è¿›è¡Œè®¨è®ºï¼**

å¼€å§‹å¯¹è¯å§ï¼
"""
        
        try:
            # Start the group discussion
            await self.user_proxy.a_initiate_chat(
                manager,
                message=initial_message
            )
            
            # Extract insights from dialogue
            dialogue_result = self._extract_dialogue_insights(group_chat.messages)
            
            return {
                'success': True,
                'original_code': func_code,
                'dialogue_history': group_chat.messages,
                'insights': dialogue_result,
                'improvement_suggestions': dialogue_result.get('improvements', []),
                'consensus_points': dialogue_result.get('consensus', []),
                'remaining_concerns': dialogue_result.get('concerns', [])
            }
            
        except Exception as e:
            logger.error(f"Dialogue evolution failed: {e}")
            return {
                'success': False,
                'error': str(e),
                'original_code': func_code
            }

    def _generate_consensus(self, func_code: str, func_name: str, task_description: str) -> Dict[str, Any]:
        """æ ¹æ®é€‰æ‹©çš„æ–¹æ³•ç”Ÿæˆå…±è¯†"""
        try:
            if self.consensus_method == "llm_analysis":
                logger.info("ä½¿ç”¨LLMåˆ†æžç”Ÿæˆå…±è¯†")
                return self.llm_generator.generate_consensus_through_llm(func_code, func_name, task_description)

            elif self.consensus_method == "multi_agent":
                logger.info("ä½¿ç”¨å¤šä»£ç†åä½œç”Ÿæˆå…±è¯†")
                return self.multi_agent_generator.generate_consensus_through_agents(func_code, func_name, task_description)

            else:  # rule_based (default)
                logger.info("ä½¿ç”¨è§„åˆ™å¼•æ“Žç”Ÿæˆå…±è¯†")
                consensus = self.rule_based_generator.generate_consensus(func_code, func_name, task_description)
                return {
                    'generation_method': 'rule_based',
                    'formatted_consensus': self.rule_based_generator.format_consensus_for_dialogue(consensus)
                }

        except Exception as e:
            logger.error(f"å…±è¯†ç”Ÿæˆå¤±è´¥: {e}")
            return {
                'generation_method': 'fallback',
                'formatted_consensus': f"""**âš ï¸ åŸºç¡€å…±è¯†ï¼š**

**å‡½æ•°**: {func_name}
**ä»»åŠ¡**: {task_description}

**è¯·åŸºäºŽå®žé™…ä»£ç è¿›è¡Œè®¨è®ºï¼**
"""
            }

    def _load_agent_consensus(self) -> Dict[str, Any]:
        """Load agent consensus from configuration file."""
        try:
            consensus_path = Path(__file__).parent.parent / "config" / "agent_consensus.json"
            if consensus_path.exists():
                with open(consensus_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            logger.warning(f"Failed to load agent consensus: {e}")

        # Return default consensus if file not found
        return {
            "technical_consensus": {
                "function_naming": {
                    "timestamp_suffix": {
                        "purpose": "Functions include timestamp/hash suffix for uniqueness",
                        "rationale": "Prevents naming conflicts and enables function evolution tracking"
                    }
                }
            }
        }

    def _format_consensus_for_dialogue(self, consensus: Dict[str, Any]) -> str:
        """Format consensus information for dialogue context."""
        formatted = "**é‡è¦æŠ€æœ¯èƒŒæ™¯å’Œå…±è¯†ï¼š**\n"

        tech_consensus = consensus.get("technical_consensus", {})

        # Function naming consensus
        naming = tech_consensus.get("function_naming", {})
        if "timestamp_suffix" in naming:
            suffix_info = naming["timestamp_suffix"]
            formatted += f"- **å‡½æ•°å‘½å**: {suffix_info.get('purpose', 'å‡½æ•°ååŒ…å«æ—¶é—´æˆ³åŽç¼€ç”¨äºŽå”¯ä¸€æ€§')}\n"

        # Type annotations consensus
        types = tech_consensus.get("type_annotations", {})
        if "any_type_usage" in types:
            any_info = types["any_type_usage"]
            formatted += f"- **è¿”å›žç±»åž‹Any**: {any_info.get('context', 'ç³»ç»Ÿè¦æ±‚ä½¿ç”¨Anyç±»åž‹ä»¥ä¿æŒçµæ´»æ€§')}\n"

        # System constraints
        constraints = consensus.get("system_constraints", {})
        if "auto_generation" in constraints:
            auto_info = constraints["auto_generation"]
            formatted += "- **ç³»ç»Ÿçº¦æŸ**: è¿™æ˜¯è‡ªåŠ¨ç”Ÿæˆçš„å‡½æ•°ï¼ŒæŸäº›è®¾è®¡é€‰æ‹©å—ç³»ç»Ÿé™åˆ¶\n"

        formatted += "\n**è¯·åŸºäºŽè¿™äº›å…±è¯†è¿›è¡Œè®¨è®ºï¼Œé¿å…å¯¹ç³»ç»Ÿè®¾è®¡é€‰æ‹©çš„æ— æ•ˆæ‰¹è¯„ã€‚**\n\n"

        return formatted
    
    def _extract_dialogue_insights(self, messages: List[Dict]) -> Dict[str, Any]:
        """Extract insights from agent dialogue."""
        
        insights = {
            'issues_raised': [],
            'improvements': [],
            'design_suggestions': [],
            'user_concerns': [],
            'consensus': [],
            'concerns': []
        }
        
        for message in messages:
            content = message.get('content', '')
            sender = message.get('name', '')
            
            # Categorize insights by agent type
            if sender == 'CodeCritic':
                # Extract criticism and issues
                if any(word in content.lower() for word in ['é—®é¢˜', 'é”™è¯¯', 'æ¼æ´ž', 'bug', 'issue']):
                    insights['issues_raised'].append({
                        'agent': sender,
                        'content': content[:200] + '...' if len(content) > 200 else content
                    })
            
            elif sender == 'CodeImprover':
                # Extract improvement suggestions
                if any(word in content.lower() for word in ['å»ºè®®', 'æ”¹è¿›', 'ä¼˜åŒ–', 'improve', 'better']):
                    insights['improvements'].append({
                        'agent': sender,
                        'content': content[:200] + '...' if len(content) > 200 else content
                    })
            
            elif sender == 'ArchitectReviewer':
                # Extract design suggestions
                if any(word in content.lower() for word in ['è®¾è®¡', 'æž¶æž„', 'design', 'architecture']):
                    insights['design_suggestions'].append({
                        'agent': sender,
                        'content': content[:200] + '...' if len(content) > 200 else content
                    })
            
            elif sender == 'UserAdvocate':
                # Extract user concerns
                if any(word in content.lower() for word in ['ç”¨æˆ·', 'ä½¿ç”¨', 'user', 'usage']):
                    insights['user_concerns'].append({
                        'agent': sender,
                        'content': content[:200] + '...' if len(content) > 200 else content
                    })
            
            elif sender == 'SynthesisCoordinator':
                # Extract consensus and final thoughts
                if any(word in content.lower() for word in ['å…±è¯†', 'æ€»ç»“', 'consensus', 'summary']):
                    insights['consensus'].append({
                        'agent': sender,
                        'content': content[:200] + '...' if len(content) > 200 else content
                    })
        
        return insights


# Factory function
def get_llm_dialogue_evolution(llm_config: Dict[str, Any]) -> LLMDialogueEvolution:
    """Get LLM dialogue evolution system instance."""
    return LLMDialogueEvolution(llm_config)


# Convenience function for quick evolution
async def evolve_code_through_dialogue(func_code: str, func_spec: Dict[str, Any], 
                                     llm_config: Dict[str, Any], 
                                     max_rounds: int = 15) -> Dict[str, Any]:
    """
    Quick code evolution using pure LLM dialogue.
    
    Args:
        func_code: Function code to evolve
        func_spec: Function specification
        llm_config: LLM configuration
        max_rounds: Maximum dialogue rounds
        
    Returns:
        Evolution result with dialogue insights
    """
    evolution_system = get_llm_dialogue_evolution(llm_config)
    return await evolution_system.evolve_through_dialogue(func_code, func_spec, max_rounds)
